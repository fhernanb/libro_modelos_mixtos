# Paquete **gamlss** 

El paquete **gamlss** de @R-gamlss se utiliza para estimar modelos en los que la variable respuesta puede tener casi cualquier distribución. Al visitar este [enlace](https://cran.r-project.org/web/packages/gamlss/) se encontrará la página de apoyo del paquete, allí se puede consultar el manual de referencia.

## Modelo lineal mixto con intercepto aleatorio

En esta sección se muestra cómo extraer los efectos fijos y la varianza del intercepto aleatorio con **gamlss**

### Ejemplo {-}
Simular $n_i=20$ observaciones para $G=10$ grupos del siguiente modelo. Luego estimar los parámetros del modelo con gamlss.

\begin{align*}
y_{ij} | b_0 &\sim  N(\mu_{ij}, \sigma^2_y) \\
\mu_{ij} &= 5 - 8 x_{ij} + b_{0i} \\
\sigma^2_y &= 15^2 \\
b_{0} &\sim N(0, \sigma^2_{b0}=10^2) \\
x_{ij} &\sim U(0, 10)
\end{align*}

```{block2, type='rmdexercise'}
Solución.
```

El código para simular los datos se muestra a continuación.

```{r}
gen_dat_b0 <- function(G, ni, beta0, beta1, sigma, sigmab0) {
  group <- rep(1:G, each=ni)
  b0 <- rep(rnorm(n=G, mean=0, sd=sigmab0), each=ni)
  x <- runif(n=G*ni, min=0, max=10)
  mu <- beta0 + beta1 * x + b0
  y <- rnorm(n=G*ni, mean=mu, sd=sigma)
  data.frame(group=group, x=x, y=y)
}

G <- 10
ni <- 20

beta0   <- 5
beta1   <- -8
sigma   <- 15
sigmab0 <- 10

theta_true <- c(beta0, beta1, sigma, sigmab0)

set.seed(5734)
datos <- gen_dat_b0(G, ni, beta0, beta1, sigma, sigmab0)
```

Para dibujar los datos se utiliza el siguiente código.

```{r gamlss01, fig.height=4, fig.width=6}
library(ggplot2)
ggplot(datos, aes(x, y, color=as.factor(group)) ) + 
  geom_point() + 
  labs(colour="Grupo/Cluster")
```

En el siguiente código se ajustan dos modelos, uno con la función `lmer` y otro con la función `gamlss`, el modelo `mod1` se utilizará como modelo de referencia para saber qué tan bien `gamlss` estima los parámetros del modelo.

```{r message=FALSE}
# con lme
require(lme4)
mod1 <- lmer(y ~ x + (1|group), data=datos)

# con gamlss
require(gamlss)
mod2 <- gamlss(y ~ x + re(random=~1|group), data=datos)
```

A continuación el código para obtener las estimaciones de $\boldsymbol{\Theta}$ con `lmer` y `gamlss`.

```{r}
vc <- VarCorr(mod1)
vc <- as.data.frame(vc)

theta_lmer <- c(fixef(mod1), vc$sdcor[2:1])

theta_gamlss <- c(coef(mod2)[1:2],
                  exp(coef(mod2, what='sigma')),
                  as.numeric(VarCorr(getSmo(mod2))[1, 2]))

cbind(theta_true, theta_lmer, theta_gamlss)
```

De la salida anterior vemos que las estimaciones con `lmer` y `gamlss` son similares y cercanas a $\boldsymbol{\Theta}$.

## Modelo lineal mixto con pendiente aleatoria

En esta sección se muestra cómo extraer los efectos fijos y la varianza de la pendiente aleatoria con **gamlss**

### Ejemplo {-}
Simular $n_i=20$ observaciones para $G=10$ grupos del siguiente modelo. Luego estimar los parámetros del modelo con gamlss.

\begin{align*}
y_{ij} | b_0 &\sim  N(\mu_{ij}, \sigma^2_y) \\
\mu_{ij} &= 5 - 3 x_{ij} + b_{0i} \\
\sigma^2_y &= 8^2 \\
b_{0} &\sim N(0, \sigma^2_{b0}=20^2) \\
x_{ij} &\sim U(0, 10)
\end{align*}

```{block2, type='rmdexercise'}
Solución.
```

El código para simular los datos se muestra a continuación.

```{r}
gen.dat <- function(G, ni, beta0, beta1, sigma, sigmab1) {
  group <- rep(1:G, each=ni)
  b1 <- rep(rnorm(n=G, mean=0, sd=sigmab1), each=ni)
  x <- runif(n=G*ni, min=0, max=10)
  mu <- beta0 + beta1 * x + b1 * x
  y <- rnorm(n=G*ni, mean=mu, sd=sigma)
  data.frame(group=group, x=x, y=y)
}

G <- 10
ni <- 20

beta0 <- 5
beta1 <- -3
sigma <- 8
sigmab1 <- 20

theta_true <- c(beta0, beta1, sigma, sigmab1)

set.seed(1234)
datos <- gen.dat(G, ni, beta0, beta1, sigma, sigmab1)
```

Para dibujar los datos se utiliza el siguiente código.

```{r gamlss02, fig.height=4, fig.width=6}
library(ggplot2)
ggplot(datos, aes(x, y, color=as.factor(group)) ) + 
  geom_point() + 
  labs(colour="Grupo/Cluster")
```

De la figura anterior se observa claramente la pendiente aleatoria.

En el siguiente código se ajustan dos modelos, uno con la función `lmer` y otro con la función `gamlss`, el modelo `mod1` se utilizará como modelo de referencia para saber qué tan bien `gamlss` estima los parámetros del modelo.

```{r message=FALSE}
# con lme
require(lme4)
mod1 <- lmer(y ~ x + (-1 + x|group), data=datos)

# con gamlss
require(gamlss)
mod2 <- gamlss(y ~ x + re(random=~-1+x|group), data=datos)
```

A continuación el código para obtener las estimaciones de $\boldsymbol{\Theta}$ con `lmer` y `gamlss`.

```{r}
vc <- VarCorr(mod1)
vc <- as.data.frame(vc)

theta_lmer <- c(fixef(mod1), vc$sdcor[2:1])

theta_gamlss <- c(coef(mod2)[1:2],
                  exp(coef(mod2, what='sigma')),
                  as.numeric(VarCorr(getSmo(mod2))[1, 2]))

cbind(theta_true, theta_lmer, theta_gamlss)
```

De la salida anterior vemos que las estimaciones con `lmer` y `gamlss` son similares y cercanas a $\boldsymbol{\Theta}$.

## Modelo lineal mixto con intercepto y pendiente aleatoria

En esta sección se muestra cómo extraer los efectos fijos y la varianza del intercepto y de la pendiente aleatoria con **gamlss**

### Ejemplo {-}
Simular $n_i=30$ observaciones para $G=20$ grupos del siguiente modelo. Luego estimar los parámetros del modelo con gamlss.


\begin{align*}
y_{ij} | b_0, b_1 &\sim N(\mu_{ij}, \sigma) \\
\mu_{ij} &= 5 - 3 x_{ij} + b_{0i} + b_{1i} x_{ij} \\
\sigma &= 8 \\
\left (
\begin{matrix}
b_{0} \\ b_{1}
\end{matrix}
\right ) &\sim
N\left ( \left [ \begin{matrix}
0 \\ 0
\end{matrix} \right ],
\left [ \begin{matrix}
\sigma^2_{b0}=40 & \sigma_{b01}=-3 \\
\sigma_{b01}=-3 & \sigma^2_{b1}=50
\end{matrix} \right ]
\right ) \\
x_{ij} &\sim U(0, 10)
\end{align*}

```{block2, type='rmdexercise'}
Solución.
```

El código para simular los datos se muestra a continuación.

```{r}
gen.dat <- function(G, ni, beta0, beta1, sigma, 
                    sigma2b0, sigma2b1, covb0b1) {
  group <- rep(1:G, each=ni)
  library(MASS) # Libreria para simular obs de Normal bivariada
  Sigma <- matrix(c(sigma2b0, covb0b1, # Matriz de var-cov
                    covb0b1, sigma2b1), ncol=2, nrow=2)
  b <- mvrnorm(n=G, mu=rep(0, 2), Sigma=Sigma) # Simulando b0 y b1
  b <- apply(b, MARGIN=2, function(c) rep(c, each=ni)) # Replicando
  b0 <- as.vector(b[, 1]) # Separando los b0
  b1 <- as.vector(b[, 2]) # Separando los b1
  x <- runif(n=G*ni, min=0, max=10)
  mu <- beta0 + beta1 * x + b0 + b1 * x
  y <- rnorm(n=G*ni, mean=mu, sd=sigma)
  data.frame(group=group, x=x, y=y)
}

G  <- 20
ni <- 30

beta0 <- 5
beta1 <- -3
sigma <- 8

sigma2b0 <- 40
sigma2b1 <- 50
covb0b1 <- -3

theta_true <- c(beta0, beta1, sigma, 
                sigma2b0, sigma2b1, covb0b1)

set.seed(123)
datos <- gen.dat(G, ni, beta0, beta1, sigma, 
                 sigma2b0, sigma2b1, covb0b1)
```

Para dibujar los datos se utiliza el siguiente código.

```{r gamlss03, fig.height=4, fig.width=6}
library(ggplot2)
ggplot(datos, aes(x, y, color=as.factor(group)) ) + 
  geom_point() + 
  labs(colour="Grupo/Cluster")
```

De la figura anterior se observa claramente la pendiente aleatoria.

En el siguiente código se ajustan dos modelos, uno con la función `lmer` y otro con la función `gamlss`, el modelo `mod1` se utilizará como modelo de referencia para saber qué tan bien `gamlss` estima los parámetros del modelo.

```{r message=FALSE}
# con lme
require(lme4)
mod1 <- NULL
mod1 <- lmer(y ~ x + (1 + x|group), data=datos)

# con gamlss
require(gamlss)
mod2 <- NULL
mod2 <- gamlss(y ~ x + re(random=~1+x|group), data=datos)
```

A continuación el código para obtener las estimaciones de $\boldsymbol{\Theta}$ con `lmer` y `gamlss`.

```{r}
# Para mod1
vc <- VarCorr(mod1)
vc <- as.data.frame(vc)

hat_sig2_b0 <- vc[1, 4]
hat_sig2_b1 <- vc[2, 4]
hat_cov_b0b1 <- vc[3, 4]
hat_sigma <- vc[4, 5]

theta_lmer <- c(fixef(mod1), hat_sigma,
                hat_sig2_b0, hat_sig2_b1, hat_cov_b0b1)

# Para mod2
aux <- getSmo(mod2)
aux <- getVarCov(aux)

hat_sig2_b0 <- aux[1, 1]
hat_sig2_b1 <- aux[2, 2]
hat_cov_b0b1 <- aux[1,2]
hat_sigma <- getSmo(mod2)$sigma

theta_gamlss <- c(coef(mod2)[1:2],
                  exp(coef(mod2, what='sigma')),
                  hat_sig2_b0, 
                  hat_sig2_b1, 
                  hat_cov_b0b1)

cbind(theta_true, theta_lmer, theta_gamlss)
```

De la salida anterior vemos que las estimaciones con `lmer` y `gamlss` son similares y cercanas a $\boldsymbol{\Theta}$.

## Ejemplo: modelo Normal con intercepto aleatorio {-}
En este ejemplo vamos a simular $n_i=50$ observaciones para $G=10$ grupos (en total 500 obs) que tengan la estructura mostrada abajo. El objetivo del ejemplo es ilustrar el uso de la función `inla` para estimar los parámetros del modelo.

\begin{align*} 
y_{ij} | b_0 &\sim  Poisson(\mu_{ij}) \\ 
\log(\mu_{ij}) &= 4 - 6 x_{ij} + b_{0i} \\
b_{0} &\sim N(0, \sigma^2_{b0}=4) \\
x_{ij} &\sim U(0, 1)
\end{align*}

El vector de parámetros de este modelo es $\boldsymbol{\Theta}=(\beta_0=4, \beta_1=-6, \sigma^2_{b0}=4)^\top$.

<div style="-moz-box-shadow: 1px 1px 3px 2px #000000;
  -webkit-box-shadow: 1px 1px 3px 2px #000000;
  box-shadow:         1px 1px 3px 2px #000000;">

```{block2, type='rmdexercise'}
Solución.
```

</div>

El código para simular las observaciones se muestra a continuación.

```{r}
ni <- 50
G <- 10
nobs <- ni * G
grupo <- factor(rep(x=1:G, each=ni))
obs <- rep(x=1:ni, times=G)
x <- runif(n=nobs, min=0, max=1)
b0 <- rnorm(n=G, mean=0, sd=sqrt(4)) # Intercepto aleatorio
b0 <- rep(x=b0, each=ni)             # El mismo intercepto aleatorio pero repetido
media <- exp(4 - 6 * x + b0)
y <- rpois(n=nobs, lambda=media)
datos <- data.frame(obs, grupo, b0, x, y)
```

Delia nos va a mostrar cómo estimar los parámetros del modelo ajustado.

```{r eval = FALSE, echo = TRUE}
library(INLA)
fit1 <- inla( )
```

## Ejemplo: modelo Poisson con intercepto y pendiente aleatoria {-}
En este ejemplo vamos a simular $n_i=50$ observaciones para $G=10$ grupos (en total 500 obs) que tengan la estructura mostrada abajo. El objetivo del ejemplo es ilustrar el uso de la función `inla` para estimar los parámetros del modelo.

\begin{align*} 
y_{ij} | b_0, b_1 &\sim Poisson(\mu_{ij}) \\ 
\log(\mu_{ij}) &= 2 - 8 x_{ij} + b_{0i} + b_{1i} x_{ij} \\
\left (
\begin{matrix}
b_{0} \\ b_{1}
\end{matrix} 
\right ) &\sim 
N\left ( \left [ \begin{matrix}
0 \\ 0
\end{matrix} \right ],
\left [ \begin{matrix}
\sigma^2_{b0}=1 & \sigma_{b01}=0.5 \\ 
\sigma_{b01} & \sigma^2_{b1}=1
\end{matrix} \right ]
\right ) \\
x_{ij} &\sim U(0, 1)
\end{align*}

El vector de parámetros de este modelo es $\boldsymbol{\Theta}=(\beta_0=2, \beta_1=-8, \sigma_{b0}^2=1, \sigma_{b1}^2=1, \sigma_{b01}=0.5)^\top$.

<div style="-moz-box-shadow: 1px 1px 3px 2px #000000;
  -webkit-box-shadow: 1px 1px 3px 2px #000000;
  box-shadow:         1px 1px 3px 2px #000000;">

```{block2, type='rmdexercise'}
Solución.
```

</div>

El código para simular las observaciones se muestra a continuación.

```{r}
ni <- 50
G <- 10
nobs <- ni * G
grupo <- factor(rep(x=1:G, each=ni))
obs <- rep(x=1:ni, times=G)
x <- runif(n=nobs, min=0, max=1)

Sigma <- matrix(c(1, 0.5, # Matriz de var-cov
                  0.5, 1), ncol=2, nrow=2)

b <- MASS::mvrnorm(n=G, mu=rep(0, 2), Sigma=Sigma)   # Simulando b0 y b1
b <- apply(b, MARGIN=2, function(c) rep(c, each=ni)) # Replicando
b0 <- as.vector(b[, 1]) # Separando los b0
b1 <- as.vector(b[, 2]) # Separando los b1

media <- exp(2 - 8 * x + b0 + b0 + b1 * x)
y <- rpois(n=nobs, lambda=media)
datos <- data.frame(obs, grupo, b0, b1, x, y)
```

Delia nos va a mostrar cómo estimar los parámetros del modelo ajustado.

```{r eval = FALSE, echo = TRUE}
library(INLA)
fit2 <- inla( )
```

